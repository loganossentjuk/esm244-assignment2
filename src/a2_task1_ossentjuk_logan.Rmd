---
title: "a2_task1_ossentjuk_logan"
author: "Logan Ossentjuk"
date: "2/1/2022"
output: 
  html_document:
    code_folding: hide
---

#Overview 
- Data description
- Questions to be addresed 
- Citation

**Data source:** Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5
Get the data: palmetto.csv
**More information and metadata:** https://portal.edirepository.org/nis/metadataviewer?packageid=edi.317.1

```{r setup, include=FALSE, message= FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(here)
library(tidyverse)
library(janitor)
library(patchwork)
library(lubridate)
library(kableExtra) 
library(ggbeeswarm)
library(GGally)
library(jtools)
library(caret)
library(AICcmodavg)
```

# Data Visualization 
2 - 3 finalized (customized, suitable for a publication) data visualizations (with figure captions) in which you explore differences in height, canopy length, canopy width, and green leaves for the two species. If you prefer, combine the figures into a compound figure using {patchwork} or {cowplot}. Below your data visualizations, add a sentence or two with a takeaway from the plots, e.g., based on these plots, which predictor variables are more likely to help classify species correctly?

```{r}
###Wrangling and subsetting 
palmetto <- read.csv(here('data', 'palmetto.csv')) %>% 
  clean_names()

palmetto_exp <- palmetto %>% 
  mutate(species = as.character(species)) %>% 
  select(height, length, width, green_lvs, species) %>% 
  drop_na() %>% 
  ggpairs(aes(color = species))
#palmetto_sub

palmetto_plot <-palmetto %>% 
  mutate(species = as.character(species)) %>% 
  select(height, length, width, green_lvs, species) %>% 
  drop_na()
```

```{r}
### Figures

p1 <- ggplot(data = palmetto_plot, aes (x = species, y = height)) + 
  geom_boxplot(aes(fill = species, width = 0.2)) + 
   scale_fill_manual(values = c("skyblue", "forestgreen"), 
                    name = "Species") +
  facet_wrap(~species) +
  theme_classic() + 
  theme(legend.position = "none") +
  labs( x = "Species", 
        y = "Height")

p2 <- ggplot(data = palmetto_plot, aes (x = species, y = green_lvs)) + 
  geom_boxplot(aes(fill = species, width = 0.2)) +
  scale_fill_manual(values = c("skyblue","forestgreen"), 
                     name = "Species") +
   facet_wrap(~species) +
   theme_classic() + 
  theme(legend.position = "none") +
  labs( x = "Species ", 
        y = "Green Leaf Percentage")

p3 <- ggplot(data = palmetto_plot, aes(x = species, y = length)) + 
  geom_boxplot(aes(fill = species, width = 0.2)) + 
   scale_fill_manual(values = c("skyblue", "forestgreen"), 
                    name = "Species") +
  facet_wrap(~species) +
  theme_classic() + 
  theme(legend.position = "none") +
  labs( x = "Species", 
        y = "Length")

p1 | p2 | p3 | 


```


# Binary Logistic Regression

Perform binary logistic regression to determine the probability of a plant being either Serenoa repens or Sabal etonia based on several predictor variables.  Perform the analysis twice, using cross validation to compare two models:
Log odds of plant type using plant height, canopy length, canopy width and green leaves as predictor variable.
Log odds of plant type using plant height, canopy width and green leaves (i.e., drop canopy length for this model)
Make sure you understand which species is the first ‘0’ factor level, and which is ‘1’ - you may want to convert to a factor first, then use the levels() function to check.  Use repeated cross validation (ten-fold cross validation, repeated at least ten times - you can use functions from the {caret} package to automate this, or manually perform the analysis using for-loops).  Based on the results of the cross validation, describe which model performs better at classification; you may wish to compare AICC values as well to support your decision. 


# Model Training 

Train your selected model using the entire dataset, and create a finalized table containing the binary logistic regression model results (at least coefficients, standard errors for the coefficients, and information for significance - consider using broom::tidy() to get you most of the way). 






